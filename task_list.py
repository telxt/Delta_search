QA_task_list = [
    'adversarialqa',
    'hotpot_qa',
    'superglue-record',

    # 'ai2_arc',
    # 'codah',
    # 'commonsense_qa',
    # 'cosmos_qa',
    # 'dream',
    # 'hellaswag',
    # 'openbookqa',
    # 'qasc',
    # 'quail',
    # 'quarel',
    # 'quartz-no_knowledge',
    # 'quartz-with_knowledge',
    # 'race-high',
    # 'race-middle',
    # 'sciq',
    # 'superglue-copa',
    # 'swag',
    # 'wino_grande',
    # 'wiqa',

    # 'boolq',
    # 'mc_taco',

    # 'eli5-askh',
    # 'eli5-asks',
    # 'eli5-eli5',
    
    # 'lama-conceptnet',
    # 'lama-google_re',
    # 'numer_sense',
    # 'search_qa',
    # 'web_questions',
]


machine_reading_comprehension = [
    'adversarialqa',
    'hotpot_qa',
    'superglue-record',
    ]

multiple_choice_qa = [
    'ai2_arc',
    'codah',
    'commonsense_qa',
    'cosmos_qa',
    'dream',
    'hellaswag',
    'openbookqa',
    'qasc',
    'quail',
    'quarel',
    'quartz-no_knowledge',
    'quartz-with_knowledge',
    'race-high',
    'race-middle',
    'sciq',
    'superglue-copa',
    'swag',
    'wino_grande',
    'wiqa',
    ]

binary = [
    'boolq',
    'mc_taco',
    ]

long_form_qa = [
    'eli5-askh',
    'eli5-asks',
    'eli5-eli5',
    ]

closed_book_qa = [
    'lama-conceptnet',
    'lama-google_re',
    'numer_sense',
    'search_qa',
    'web_questions',
    ]


quan_task = [
    'adversarialqa',
    'hotpot_qa',
    'superglue-record',

    'ai2_arc',
    'codah',
    'commonsense_qa',
    'cosmos_qa',
    'dream',
    'hellaswag',
    'openbookqa',
    'qasc',
    'quail',
    'quarel',
    'quartz-no_knowledge',
    'quartz-with_knowledge',
    'race-high',
    'race-middle',
    'sciq',
    'superglue-copa',
    'swag',
    'wino_grande',
    'wiqa',

    'boolq',
    'mc_taco',

    'eli5-askh',
    'eli5-asks',
    'eli5-eli5',
    
    'lama-conceptnet',
    'lama-google_re',
    'numer_sense',
    'search_qa',
    'web_questions',


    'amazon_polarity',
    'financial_phrasebank',
    'anli',
    'scitail',
    'climate_fever',
    'health_fact',
    'liar',
    'tab_fact',
    'tweet_eval-offensive',
    'tweet_eval-sentiment',
    'tweet_eval-irony',
    'glue-mrpc',
    'glue-qqp',
    'medical_questions_pairs',
    'samsum',
    'xsum',
    'blimp-ellipsis_n_bar_1',
    'blimp-irregular_past_participle_adjectives',
    'blimp-sentential_negation_npi_scope',
]

